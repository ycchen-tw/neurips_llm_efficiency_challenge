{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://github.com/QwenLM/Qwen/blob/main/finetune.py#L338\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "import math\n",
    "import logging\n",
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '5'\n",
    "from typing import Dict, Optional, List\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from transformers import Trainer, GPTQConfig, deepspeed, DataCollatorForSeq2Seq, TrainingArguments\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.trainer_pt_utils import LabelSmoother\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
    "from accelerate.utils import DistributedType\n",
    "from datasets import load_dataset\n",
    "from lion_pytorch import Lion\n",
    "\n",
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"ycchen/yc-test1\"\n",
    "NUM_EPOCH = 5\n",
    "BATCH_SIZE = 1\n",
    "GRAD_ACC_STEPS = 32\n",
    "MAX_LEN = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oasst_seed_dataset = load_dataset('ycchen_submission_3_dataset', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=True)\n",
    "tokenizer.pad_token = '<|extra_21|>'\n",
    "tokenizer.pad_token_id = tokenizer('<|extra_21|>').input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(\n",
    "    source,\n",
    "    system_message: str = \"Answer in the style of an AI Assistant.\"\n",
    "):  \n",
    "    if source['source'] == 'oasst':\n",
    "        system_message = \"Answer in the style of an AI Assistant.\\n\"\n",
    "    elif source['source'] == 'arc':\n",
    "        system_message = \"The following are multiple choice questions (with answers)\\n\"\n",
    "    else:\n",
    "        system_message =  \"Answer with knowledge from web search.\\n\"\n",
    "    \n",
    "    input_ids, targets = [], []\n",
    "    system_text = system_message + '\\n'\n",
    "    system = tokenizer(system_text).input_ids\n",
    "    input_ids += system\n",
    "    targets += [IGNORE_TOKEN_ID] * len(system)\n",
    "\n",
    "    for i_round in range(0, len(source['conversations'])//2, 2):\n",
    "        question_text = source['conversations'][i_round]\n",
    "        answer_text =  source['conversations'][i_round+1]\n",
    "\n",
    "        question = tokenizer(question_text).input_ids\n",
    "        input_ids += question\n",
    "        targets += [IGNORE_TOKEN_ID] * len(question)\n",
    "\n",
    "        answer = tokenizer(answer_text).input_ids\n",
    "        input_ids += answer\n",
    "        targets += answer\n",
    "\n",
    "    input_ids = input_ids[:MAX_LEN]\n",
    "    targets = targets[:MAX_LEN]\n",
    "\n",
    "    input_ids = torch.tensor(input_ids, dtype=torch.int64)\n",
    "    targets = torch.tensor(targets, dtype=torch.int64)\n",
    "\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=targets,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = oasst_seed_dataset.map(preprocess, remove_columns=['conversations', 'source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map = \"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    "    use_flash_attn=False,\n",
    ").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_gradient_checkpointing = True\n",
    "model = prepare_model_for_kbit_training(\n",
    "    model, use_gradient_checkpointing=use_gradient_checkpointing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = []\n",
    "for i_layer in range(0, 40):\n",
    "    target_modules.extend([\n",
    "        f\"{i_layer}.attn.c_attn\", f\"{i_layer}.attn.c_proj\",\n",
    "        f\"{i_layer}.mlp.c_proj\", f\"{i_layer}.mlp.w1\", f\"{i_layer}.mlp.w2\",\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=16,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print peft trainable params\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Lion(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4, weight_decay=0.1)\n",
    "num_samples = len(train_dataset)\n",
    "num_training_steps = num_samples * NUM_EPOCH / (BATCH_SIZE * GRAD_ACC_STEPS)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_training_steps=num_training_steps,\n",
    "    num_warmup_steps=num_training_steps*0.06,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = TrainingArguments(\n",
    "    \"ycchen_submission_3_qwen_qlora\",\n",
    "    num_train_epochs=NUM_EPOCH,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRAD_ACC_STEPS,\n",
    "    ddp_find_unused_parameters=False,\n",
    "    logging_steps=1,\n",
    "    save_strategy='epoch',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    args=train_args,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=train_dataset,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
