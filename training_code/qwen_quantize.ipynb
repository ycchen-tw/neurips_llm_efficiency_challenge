{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_dir = \"Qwen/Qwen-14B\"\n",
    "quantized_model_dir = \"Qwen-14B-8bit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wikitext2(tokenizer):\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    import random\n",
    "    wikidata = load_dataset('wikitext', 'wikitext-2-raw-v1', split='test')\n",
    "    wikilist = [' \\n' if s == '' else s for s in wikidata['text'] ]\n",
    "\n",
    "    text = ''.join(wikilist)\n",
    "    trainenc = tokenizer(text, return_tensors='pt')\n",
    "\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    torch.random.manual_seed(0)\n",
    "\n",
    "    traindataset = []\n",
    "\n",
    "    num_example = 120\n",
    "    seqlen = 4096\n",
    "\n",
    "    for _ in range(num_example):\n",
    "        i = random.randint(0, trainenc.input_ids.shape[1] - seqlen - 1)\n",
    "        j = i + seqlen\n",
    "        inp = trainenc.input_ids[:, i:j]\n",
    "        attention_mask = torch.ones_like(inp)\n",
    "        traindataset.append({'input_ids':inp,'attention_mask': attention_mask})\n",
    "    return traindataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_dir, trust_remote_code=True, use_fast=True)\n",
    "examples = get_wikitext2(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_config = BaseQuantizeConfig(\n",
    "    bits=8,  # quantize model to 8-bit\n",
    "    group_size=128,  # it is recommended to set the value to 128\n",
    "    desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = AutoGPTQForCausalLM.from_pretrained(\n",
    "    pretrained_model_dir,\n",
    "    quantize_config,\n",
    "    # device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    # max_memory={0: \"22GIB\", 1: \"22GIB\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantize model, the examples should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "model.quantize(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save quantized model\n",
    "model.save_pretrained(quantized_model_dir+'-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
